{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a94a2190-5707-4c53-8b1d-412b6c9c6d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluate trained models on the official CUB test set\n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import joblib\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from scipy.special import softmax\n",
    "sys.path.append('/home/mattyshen/interpretableDistillation')\n",
    "from interpretDistill import fourierDistill\n",
    "from interpretDistill import mo_figs\n",
    "sys.path.append('/home/mattyshen/iCBM')\n",
    "\n",
    "from CUB.dataset import load_data\n",
    "from CUB.config import BASE_DIR, N_CLASSES, N_ATTRIBUTES, DEVICE, get_device, set_device\n",
    "from analysis import AverageMeter, multiclass_metric, accuracy, binary_accuracy\n",
    "\n",
    "from imodels import FIGSClassifierCV, FIGSRegressorCV, FIGSRegressor\n",
    "\n",
    "\n",
    "class ARGS:\n",
    "    def __init__(self, a_dict):\n",
    "        for k in a_dict.keys():\n",
    "            exec(f'self.{k} = a_dict[\"{k}\"]')\n",
    "            \n",
    "parser_args = ['log_dir', \n",
    "               'model_dirs', \n",
    "               'model_dirs2', \n",
    "               'eval_data', \n",
    "               'use_attr', \n",
    "               'no_img', \n",
    "               'bottleneck', \n",
    "               'image_dir', \n",
    "               'n_class_attr', \n",
    "               'data_dir', \n",
    "               'n_attributes', \n",
    "               'attribute_group',\n",
    "               'feature_group_results', \n",
    "               'use_relu', \n",
    "               'use_sigmoid', \n",
    "               'use_gbsm', \n",
    "               'expand_gbsm_dim', \n",
    "               'gpu']\n",
    "parser_sigmoid = ['/home/mattyshen/iCBM/CUB/eval/JointSigmoidModels/outputs', \n",
    "               ['/home/mattyshen/iCBM/CUB/best_models/Joint0.01SigmoidModel__Seed1/outputs/best_model_1.pth', '/home/mattyshen/iCBM/CUB/best_models/Joint0.01SigmoidModel__Seed2/outputs/best_model_2.pth', '/home/mattyshen/iCBM/CUB/best_models/Joint0.01SigmoidModel__Seed3/outputs/best_model_3.pth'],\n",
    "               None,\n",
    "               'test',\n",
    "               True,\n",
    "               False,\n",
    "               False,\n",
    "               'images',\n",
    "               2,\n",
    "               'CUB_processed/class_attr_data_10',\n",
    "               112,\n",
    "               None,\n",
    "               False,\n",
    "               False,\n",
    "               True,\n",
    "               False,\n",
    "               False,\n",
    "               2]\n",
    "parser_gbsm = ['/home/mattyshen/iCBM/CUB/eval/JointGBSMModels/outputs', \n",
    "               ['/home/mattyshen/iCBM/CUB/best_models/Joint0.01GBSMModel__Seed1/outputs/best_model_1.pth', '/home/mattyshen/iCBM/CUB/best_models/Joint0.01GBSMModel__Seed2/outputs/best_model_2.pth', '/home/mattyshen/iCBM/CUB/best_models/Joint0.01GBSMModel__Seed3/outputs/best_model_3.pth'],\n",
    "               None,\n",
    "               'test',\n",
    "               True,\n",
    "               False,\n",
    "               False,\n",
    "               'images',\n",
    "               2,\n",
    "               'CUB_processed/class_attr_data_10',\n",
    "               112,\n",
    "               None,\n",
    "               False,\n",
    "               False,\n",
    "               True,\n",
    "               True,\n",
    "               False,\n",
    "               2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11af1cc4-e7c5-43cd-a49d-559f0b01ce51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.ARGS object at 0x7fac046d17c0>\n"
     ]
    }
   ],
   "source": [
    "args_dict = dict(zip(parser_args, parser_sigmoid))\n",
    "torch.backends.cudnn.benchmark=True\n",
    "args = ARGS(args_dict)\n",
    "\n",
    "set_device(args.gpu)\n",
    "\n",
    "args.three_class = (args.n_class_attr == 3)\n",
    "args.batch_size = 16\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfcda1f0-6257-4e00-a125-509763bcafc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CUB_processed/class_attr_data_10'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb4d0365-e88b-4a2b-b11d-75a9bfca2667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/mattyshen/iCBM/CUB/best_models/Joint0.01SigmoidModel__Seed1/outputs/best_model_1.pth'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.model_dirs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49d94884-4234-453d-b813-0cee39a2af9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_FT_data(args, data='trainval', override_train = True, batch_size = 32):\n",
    "    #TODO: loop over all models\n",
    "    model = torch.load(args.model_dirs[0])\n",
    "    model = model.to(get_device())\n",
    "    model.eval()\n",
    "    # print(model.first_model)\n",
    "    # print(model.sec_model)\n",
    "    # print(model.sec_model.expand_dim)\n",
    "    with torch.no_grad():\n",
    "        if data == 'test':\n",
    "            test_dir = os.path.join(BASE_DIR, args.data_dir, 'test.pkl')\n",
    "            print(test_dir)\n",
    "            loader = load_data([test_dir], args.use_attr, args.no_img, batch_size, image_dir=args.image_dir,\n",
    "                               n_class_attr=args.n_class_attr, override_train=override_train)\n",
    "        else:\n",
    "            train_dir = os.path.join(BASE_DIR, args.data_dir, 'train.pkl')\n",
    "            print(train_dir)\n",
    "            val_dir = os.path.join(BASE_DIR, args.data_dir, 'val.pkl')\n",
    "            print(args.use_attr, args.no_img, batch_size, args.image_dir,args.n_class_attr)\n",
    "            loader = load_data([train_dir, val_dir], args.use_attr, args.no_img, batch_size, image_dir=args.image_dir,\n",
    "                               n_class_attr=args.n_class_attr, override_train=override_train)\n",
    "        attrs_true = []\n",
    "        attrs_hat = []\n",
    "        labels_true = []\n",
    "        labels_hat = []\n",
    "        for data_idx, data in enumerate(loader):\n",
    "            if data_idx % 50 == 0: \n",
    "                print(f'loading data index: {data_idx}...')\n",
    "            inputs, labels, attr_labels = data\n",
    "            attr_labels = torch.stack(attr_labels).t()\n",
    "\n",
    "            inputs_var = torch.autograd.Variable(inputs).to(get_device())\n",
    "            labels_var = torch.autograd.Variable(labels).to(get_device())\n",
    "            #labels = labels.to(get_device()) if torch.cuda.is_available() else labels\n",
    "\n",
    "            print(inputs_var.shape)\n",
    "            outputs = model(inputs_var)\n",
    "            print(len(outputs))\n",
    "            class_outputs = outputs[0]\n",
    "            print(type(class_outputs))\n",
    "            \n",
    "            attr_outputs = [torch.nn.Sigmoid()(o) for o in outputs[1:]]\n",
    "            attr_outputs_sigmoid = attr_outputs\n",
    "\n",
    "            attrs_hat.append(torch.stack(attr_outputs).squeeze(2).detach().cpu().numpy())\n",
    "            attrs_true.append(attr_labels.T)\n",
    "            labels_hat.append(class_outputs.detach().cpu().numpy())\n",
    "            labels_true.append(labels)\n",
    "\n",
    "\n",
    "        # X_train = pd.DataFrame(np.concatenate(attrs_hat, axis=0) > p_thresh, columns = [f'c{i}' for i in range(1, 113)]).astype(np.int64)\n",
    "        X_hat = pd.DataFrame(np.concatenate(attrs_hat, axis=1).T, columns = [f'c{i}' for i in range(1, 113)])\n",
    "        X = pd.DataFrame(np.concatenate(attrs_true, axis = 1).T, columns = [f'c{i}' for i in range(1, 113)])\n",
    "\n",
    "        y = pd.Series(np.concatenate([l.numpy().reshape(-1, ) for l in labels_true]))\n",
    "        y_hat = pd.DataFrame(np.concatenate(labels_hat, axis = 0))\n",
    "\n",
    "        del attrs_hat\n",
    "        del labels\n",
    "        del labels_hat\n",
    "        del loader\n",
    "        del data\n",
    "        del inputs\n",
    "        # del labels\n",
    "        # del attr_labels\n",
    "        del outputs\n",
    "        del class_outputs\n",
    "        del attr_outputs\n",
    "        del attr_outputs_sigmoid\n",
    "        del inputs_var\n",
    "        del labels_var\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return X_hat, X, y_hat, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c3d2da3-2c2c-4c84-b710-0f9a6e7ddc24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False, 'images', 2, 'CUB_processed/class_attr_data_10')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.use_attr, args.no_img, args.image_dir, args.n_class_attr, args.data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d7eb8f8-770b-4ee1-9cfc-91b15caf7152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mattyshen/iCBM/CUB/CUB_processed/class_attr_data_10/train.pkl\n",
      "True False 32 images 2\n",
      "loading data index: 0...\n",
      "torch.Size([32, 3, 299, 299])\n",
      "113\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 299, 299])\n",
      "113\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 299, 299])\n",
      "113\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 299, 299])\n",
      "113\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 299, 299])\n",
      "113\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 299, 299])\n",
      "113\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 299, 299])\n",
      "113\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 299, 299])\n",
      "113\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 299, 299])\n",
      "113\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 299, 299])\n",
      "113\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 299, 299])\n",
      "113\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 299, 299])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X_train_hat, X_train, y_train_hat, y_train \u001b[38;5;241m=\u001b[39m \u001b[43mget_FT_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverride_train\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m X_test_hat, X_test, y_test_hat, y_test \u001b[38;5;241m=\u001b[39m get_FT_data(args, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36mget_FT_data\u001b[0;34m(args, data, override_train, batch_size)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#labels = labels.to(get_device()) if torch.cuda.is_available() else labels\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(inputs_var\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 37\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_var\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(outputs))\n\u001b[1;32m     39\u001b[0m class_outputs \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/iCBM/CUB/template_model.py:62\u001b[0m, in \u001b[0;36mEnd2EndModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sec_model_outputs, sec_model_aux_outputs\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 62\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfirst_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_stage2(outputs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/iCBM/CUB/template_model.py:217\u001b[0m, in \u001b[0;36mInception3.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    215\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMixed_6d(x)\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# N x 768 x 17 x 17\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMixed_6e\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# N x 768 x 17 x 17\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maux_logits:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/iCBM/CUB/template_model.py:365\u001b[0m, in \u001b[0;36mInceptionC.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    363\u001b[0m branch7x7 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbranch7x7_1(x)\n\u001b[1;32m    364\u001b[0m branch7x7 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbranch7x7_2(branch7x7)\n\u001b[0;32m--> 365\u001b[0m branch7x7 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbranch7x7_3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbranch7x7\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    367\u001b[0m branch7x7dbl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbranch7x7dbl_1(x)\n\u001b[1;32m    368\u001b[0m branch7x7dbl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbranch7x7dbl_2(branch7x7dbl)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/iCBM/CUB/template_model.py:504\u001b[0m, in \u001b[0;36mBasicConv2d.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 504\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(x)\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mrelu(x, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train_hat, X_train, y_train_hat, y_train = get_FT_data(args, override_train = True)\n",
    "\n",
    "X_test_hat, X_test, y_test_hat, y_test = get_FT_data(args, data = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7038db2-ebc8-4fc4-ae25-219d14749ca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>c3</th>\n",
       "      <th>c4</th>\n",
       "      <th>c5</th>\n",
       "      <th>c6</th>\n",
       "      <th>c7</th>\n",
       "      <th>c8</th>\n",
       "      <th>c9</th>\n",
       "      <th>c10</th>\n",
       "      <th>...</th>\n",
       "      <th>c103</th>\n",
       "      <th>c104</th>\n",
       "      <th>c105</th>\n",
       "      <th>c106</th>\n",
       "      <th>c107</th>\n",
       "      <th>c108</th>\n",
       "      <th>c109</th>\n",
       "      <th>c110</th>\n",
       "      <th>c111</th>\n",
       "      <th>c112</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.997168</td>\n",
       "      <td>0.002838</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.997925</td>\n",
       "      <td>0.997404</td>\n",
       "      <td>0.070798</td>\n",
       "      <td>0.000639</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.003451</td>\n",
       "      <td>0.998567</td>\n",
       "      <td>0.008230</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.002673</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.002072</td>\n",
       "      <td>0.996889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>0.998915</td>\n",
       "      <td>0.999547</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.995172</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.003336</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.001520</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.065626</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000608</td>\n",
       "      <td>0.000702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000298</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.006749</td>\n",
       "      <td>0.996593</td>\n",
       "      <td>0.002590</td>\n",
       "      <td>0.002136</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.996558</td>\n",
       "      <td>0.995859</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.991023</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>0.000848</td>\n",
       "      <td>0.000850</td>\n",
       "      <td>0.998815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.999575</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>0.000342</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.004145</td>\n",
       "      <td>0.999393</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000656</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.998703</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.997077</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.002653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000348</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.991590</td>\n",
       "      <td>0.010792</td>\n",
       "      <td>0.269908</td>\n",
       "      <td>0.002203</td>\n",
       "      <td>0.000559</td>\n",
       "      <td>0.004344</td>\n",
       "      <td>0.002673</td>\n",
       "      <td>0.001327</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>0.894389</td>\n",
       "      <td>0.000974</td>\n",
       "      <td>0.014078</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.002349</td>\n",
       "      <td>0.002375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5979</th>\n",
       "      <td>0.000608</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>0.002528</td>\n",
       "      <td>0.009372</td>\n",
       "      <td>0.004716</td>\n",
       "      <td>0.002445</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.980387</td>\n",
       "      <td>0.017051</td>\n",
       "      <td>0.002176</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>0.000819</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.981107</td>\n",
       "      <td>0.002328</td>\n",
       "      <td>0.955899</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>0.002906</td>\n",
       "      <td>0.005129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5980</th>\n",
       "      <td>0.000370</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.074480</td>\n",
       "      <td>0.996344</td>\n",
       "      <td>0.003308</td>\n",
       "      <td>0.998139</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>0.006045</td>\n",
       "      <td>0.001480</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.005574</td>\n",
       "      <td>0.000653</td>\n",
       "      <td>0.004495</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.002586</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.005962</td>\n",
       "      <td>0.002283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5981</th>\n",
       "      <td>0.000559</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.003472</td>\n",
       "      <td>0.001571</td>\n",
       "      <td>0.997409</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.995713</td>\n",
       "      <td>0.995884</td>\n",
       "      <td>0.999127</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.997997</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.002126</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.001449</td>\n",
       "      <td>0.999766</td>\n",
       "      <td>0.000739</td>\n",
       "      <td>0.001646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5982</th>\n",
       "      <td>0.000396</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.001485</td>\n",
       "      <td>0.999044</td>\n",
       "      <td>0.001498</td>\n",
       "      <td>0.001283</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.998458</td>\n",
       "      <td>0.000801</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999396</td>\n",
       "      <td>0.001065</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.002833</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.003661</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.002528</td>\n",
       "      <td>0.998108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5983</th>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.960608</td>\n",
       "      <td>0.003889</td>\n",
       "      <td>0.001220</td>\n",
       "      <td>0.001965</td>\n",
       "      <td>0.001205</td>\n",
       "      <td>0.952817</td>\n",
       "      <td>0.995308</td>\n",
       "      <td>0.001560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.996477</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>0.002625</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.998453</td>\n",
       "      <td>0.002442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5984 rows Ã— 112 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            c1        c2        c3        c4        c5        c6        c7  \\\n",
       "0     0.000029  0.000049  0.997168  0.002838  0.001316  0.997925  0.997404   \n",
       "1     0.000127  0.000002  0.000452  0.998915  0.999547  0.000025  0.000003   \n",
       "2     0.000298  0.000297  0.006749  0.996593  0.002590  0.002136  0.000303   \n",
       "3     0.999575  0.000173  0.000532  0.000563  0.000342  0.000943  0.000027   \n",
       "4     0.000348  0.000191  0.991590  0.010792  0.269908  0.002203  0.000559   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5979  0.000608  0.000457  0.002528  0.009372  0.004716  0.002445  0.000050   \n",
       "5980  0.000370  0.000215  0.074480  0.996344  0.003308  0.998139  0.000445   \n",
       "5981  0.000559  0.000039  0.003472  0.001571  0.997409  0.000278  0.000051   \n",
       "5982  0.000396  0.000026  0.001485  0.999044  0.001498  0.001283  0.000260   \n",
       "5983  0.000372  0.000067  0.960608  0.003889  0.001220  0.001965  0.001205   \n",
       "\n",
       "            c8        c9       c10  ...      c103      c104      c105  \\\n",
       "0     0.070798  0.000639  0.000169  ...  0.000181  0.000144  0.003451   \n",
       "1     0.995172  0.000221  0.000145  ...  0.000041  0.003336  0.000543   \n",
       "2     0.996558  0.995859  0.000817  ...  0.000150  0.000055  0.000132   \n",
       "3     0.004145  0.999393  0.000163  ...  0.000018  0.000656  0.000053   \n",
       "4     0.004344  0.002673  0.001327  ...  0.000348  0.000264  0.000388   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5979  0.980387  0.017051  0.002176  ...  0.000265  0.000819  0.002930   \n",
       "5980  0.006045  0.001480  0.000161  ...  0.000126  0.001359  0.005574   \n",
       "5981  0.995713  0.995884  0.999127  ...  0.000085  0.997997  0.000627   \n",
       "5982  0.998458  0.000801  0.000231  ...  0.999396  0.001065  0.000278   \n",
       "5983  0.952817  0.995308  0.001560  ...  0.000414  0.000229  0.996477   \n",
       "\n",
       "          c106      c107      c108      c109      c110      c111      c112  \n",
       "0     0.998567  0.008230  0.000151  0.002673  0.000009  0.002072  0.996889  \n",
       "1     0.000009  0.001520  0.000108  0.065626  0.000092  0.000608  0.000702  \n",
       "2     0.000264  0.991023  0.000146  0.000867  0.000848  0.000850  0.998815  \n",
       "3     0.000009  0.998703  0.000081  0.997077  0.000083  0.000556  0.002653  \n",
       "4     0.000867  0.894389  0.000974  0.014078  0.000202  0.002349  0.002375  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "5979  0.000084  0.981107  0.002328  0.955899  0.000504  0.002906  0.005129  \n",
       "5980  0.000653  0.004495  0.000172  0.002586  0.000093  0.005962  0.002283  \n",
       "5981  0.000072  0.002126  0.000085  0.001449  0.999766  0.000739  0.001646  \n",
       "5982  0.000035  0.002833  0.000043  0.003661  0.000085  0.002528  0.998108  \n",
       "5983  0.000388  0.002625  0.000034  0.000161  0.000012  0.998453  0.002442  \n",
       "\n",
       "[5984 rows x 112 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cb599a8-d0dd-4829-9d61-34e3acf25564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7747670003451846"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_test_hat.idxmax(axis = 1)== y_test.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c333bbc3-5390-4890-b039-1ee70b418f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(y_train_hat1.idxmax(axis = 1)== y_train1.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55e081b-d99e-44a0-975e-a5bd697233cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(y_test_hat1.idxmax(axis = 1)== y_test1.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76acc6f-ac61-4d88-9f3b-fe4cb0c2f4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_threshold(y_true, y_probs):\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_true, y_probs)\n",
    "    f1_scores = 2 * (precisions * recalls) / (precisions + recalls)\n",
    "    optimal_idx = np.argmax(f1_scores)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    return optimal_threshold\n",
    "\n",
    "optimal_thresholds = []\n",
    "\n",
    "for class_idx in range(112):\n",
    "    y_true_class = X_train.iloc[:, class_idx]\n",
    "    y_probs_class = X_train_hat.iloc[:, class_idx]\n",
    "    optimal_thresholds.append(find_optimal_threshold(y_true_class, y_probs_class))\n",
    "    \n",
    "optimal_thresholds = np.array(optimal_thresholds)\n",
    "\n",
    "y_train_probs_hat = pd.DataFrame(softmax(y_train_hat, axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83d26dd-22c6-4126-bb4d-80414c3b35ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sys.path.append('/home/mattyshen/interpretableDistillation')\n",
    "from interpretDistill import fourierDistill\n",
    "from interpretDistill.mo_figs import FIGSHydraRegressor\n",
    "sys.path.append('/home/mattyshen/iCBM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d561275-b246-4474-b388-112195945e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "figs_r = FIGSHydraRegressor(max_trees = 3, max_rules = 10, max_depth = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0893f1a9-40fb-48b8-966b-9290990fd748",
   "metadata": {},
   "outputs": [],
   "source": [
    "figs_r.fit(X_train_hat, softmax(y_train_hat, axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f3e299-4315-4a27-aadb-d695c82d835e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.argmax(figs_r.predict(X_train_hat), axis = 1) == y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e5a315-ad96-49c3-b33a-540ade53f3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.argmax(figs_r.predict(X_test_hat), axis = 1) == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41998ab1-c0db-4798-839b-907b494cd3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "figs_r_no_sm = mo_figs.FIGSRegressor(max_trees = 5, max_rules = 20, max_depth = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75480a99-beea-44ff-8ba7-f17f33eb259b",
   "metadata": {},
   "outputs": [],
   "source": [
    "figs_r_no_sm.fit(X_train_hat, y_train_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99fd461-7ebb-48c5-a985-25de52a71810",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.argmax(figs_r_no_sm.predict(X_train_hat), axis = 1) == y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d68e783-dfb4-4f1a-8ef8-ca4054a8b1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.argmax(figs_r_no_sm.predict(X_test_hat), axis = 1) == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00df047-a5a5-443c-bee7-05e0bb5475c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "figs_c = mo_figs.FIGSClassifier(max_trees = 30, max_rules = 90, max_depth = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc8f78d-1a99-482a-853b-8e99aa9dfec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "figs_c.fit(X_train_hat, pd.DataFrame(y_train_hat.idxmax(axis = 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb5791f-6606-48f1-8870-de54311303e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9306bcc2-1cfc-4441-8ac7-70f8b9620b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(figs_c.predict(X_train_hat).reshape(-1, ) == y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658923d1-648d-4a3b-8c03-cdab75415992",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(figs_c.predict(X_test_hat).reshape(-1, ) == y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae40bc7-e05e-401a-8d49-94c4b2cce7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(1, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bf8b68-1782-46a5-9a0f-886e457a2860",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.mean(t == (X_hat > optimal_thresholds).astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb98a49a-6a2e-42b5-86d1-dccc4637abb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, y_train_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52182656-8a17-4079-876c-8a9ba8817bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_probs = pd.DataFrame(softmax(y_train_hat, axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6085dc18-000c-45ca-972f-3d062cddc397",
   "metadata": {},
   "outputs": [],
   "source": [
    "figs_models = []\n",
    "for c in range(200):\n",
    "    if c%25 == 0:\n",
    "        print(c)\n",
    "    figs = FIGSRegressor(max_trees = 3, max_rules = 10)\n",
    "    figs.fit(X_train, y_hat_probs[c])\n",
    "    figs_models.append(figs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665ce95f-4f0c-4b4d-ad1e-beb5ad00ab20",
   "metadata": {},
   "outputs": [],
   "source": [
    "figs_models[123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7532ead3-1a74-4cc1-adf6-c5fa7d50c973",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [f.predict(X_train) for f in figs_models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2856e98-8859-4118-8e41-a9cb162c57a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.argmax(np.array(preds).T, axis = 1) == y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b21f3a-32b9-43a1-9dca-bad09ff599c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(y_train_hat.idxmax(axis = 1) == y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4431c3-fdf7-4b9f-bf46-83e105434bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test, y_test_hat = get_FT_data(args, data = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3a371d-ea57-4d56-9521-265f24fd22e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = [f.predict(X_test) for f in figs_models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029fcfb5-301e-46ec-8eb9-cfa8100e3dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.argmax(np.array(test_preds).T, axis = 1) == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc4ee4a-4318-40ed-affa-0cd4bc6f4c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(y_test_hat.idxmax(axis = 1) == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53904fb1-6b7c-4ca0-8a3b-14ddea0dd84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "figs_models2 = []\n",
    "for i in range(145, 200):\n",
    "    if i%25 == 0:\n",
    "        print(f'training class {i}')\n",
    "    figs_i = FIGSClassifierCV(n_rules_list = [20, 20], n_trees_list = [5, 10])\n",
    "    figs_i.fit(X_train, np.where(y_train_hat == i, 1, 0))\n",
    "    figs_models.append(figs_i)\n",
    "    #np.where(y_train_hat == 46, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec3e52d-815a-4127-98f2-ba02decc8b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.where(y_train_hat == 147, 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618a9c3f-9151-4737-9d93-320b370a89eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "figs_i = FIGSClassifierCV(n_rules_list = [20, 20], n_trees_list = [5, 10])\n",
    "figs_i.fit(X_train, np.where(y_train_hat == i, 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9e01b7-900c-431f-bad2-ddeb228c3499",
   "metadata": {},
   "outputs": [],
   "source": [
    "figs_models = []\n",
    "for i in range(0, 200):\n",
    "    if i%25 == 0:\n",
    "        print(f'training class {i}')\n",
    "    figs_i = FIGSClassifierCV(n_rules_list = [20, 20], n_trees_list = [5, 10])\n",
    "    if np.sum(np.where(y_train_hat == i, 1, 0)) > 0:\n",
    "        figs_i.fit(X_train, np.where(y_train_hat == i, 1, 0))\n",
    "    figs_models.append(figs_i)\n",
    "    #np.where(y_train_hat == 46, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d073bb6f-4215-47b1-857b-c71726ef73fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(figs_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa58d6a0-267b-4722-a196-a14d579f51c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for i in figs_models:\n",
    "    if hasattr(i, 'figs'):\n",
    "        print(i.predict_proba(X_train)[:, 0].shape)\n",
    "        predictions.append(i.predict_proba(X_train)[:, 0])\n",
    "    else:\n",
    "        print(np.zeros((X_train.shape[0], )).shape)\n",
    "        predictions.append(np.zeros((X_train.shape[0], )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f2f172-4480-4f14-976c-2e13de9b4ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(np.vstack((predictions)).T, axis = 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a024b4-7e3c-4401-b3b4-7afe3b52d7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(np.vstack((predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe5a93b-087f-4a74-8d7d-31a53cf0befb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.argmax(np.vstack((predictions)).T, axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f768e334-eaff-442d-a402-b1a7a43ac19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.argmax(np.vstack((predictions)).T, axis = 1) == y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde43be7-7811-4192-9abb-20e2630d9947",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate(predictions, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95841dd-efeb-4e67-be21-c125a1478d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "figs_models[i].predict_proba(X_train).shape, figs_models[i].predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c87bdd-c483-4fda-aa5e-f4371332f688",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(y_train_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac0fe31-92ec-42c6-b919-df91c39a1e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(y_train_hat == y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b960098-4ca1-42ed-9932-2a5c7fc06a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "figs = FIGSClassifierCV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d07ca97-2107-4a33-9b9c-79aac3bd8d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "figs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3df793-05b2-439c-8e7a-583a452c7e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "figs_distill = FIGSClassifierCV()\n",
    "figs_distill.fit(X_train, y_train_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3107e760-d29d-4ef6-a693-592344a5d109",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(figs_distill.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca046415-082b-4d24-b547-b178f83a4816",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(figs.predict(X_train) == y_train), np.mean(figs_distill.predict(X_train) == y_train), np.mean(figs_distill.predict(X_train) == y_train_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32f411d-4461-4612-a4f3-800fd465196f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftd = fourierDistill.FTDistillClassifierCV(pre_interaction='l0l2', \n",
    "                             pre_max_features=50,\n",
    "                             post_interaction='l0l2', \n",
    "                             post_max_features=20,\n",
    "                             size_interactions=3,  \n",
    "                             cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90498967-ddc5-40bb-b91b-42ff08403e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c5fc94-cebb-422b-99b5-1634b384949a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(ftd.predict(X_train) == y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9832d2f-3f8d-4d81-bff5-c4e0d26e9e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = get_FT_data(args, data='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748fad9f-1b59-493f-9e3a-6421e6986bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = pd.Series(np.concatenate([l.numpy().reshape(-1, ) for l in y_test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945c73fc-030d-474a-88f4-1af3fc8a059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(np.sum(ftd.post_sparsity_model.coef_ != 0, axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ea4559-3221-4f7c-ae94-1cd515974054",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.data.extend(pickle.load(open(file_path, 'rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7072c5-5a84-4573-8d21-81a549e64b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(ftd.predict(X_train) == y_train), np.mean(ftd.predict(X_test) == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a9a10a-abda-415c-ba85-d355f02561de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "data = pickle.load(open('/home/mattyshen/iCBM/CUB/CUB_processed/class_attr_data_10/train.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07d9186-5e24-4e31-bb76-09b5f96995c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"input.txt\"  # Replace with the path to your original file\n",
    "output_file = \"filtered_output.txt\"  # The file to save the filtered lines\n",
    "\n",
    "# List of numbers to filter\n",
    "numbers_to_keep =     [1, 4, 6, 7, 10, 14, 15, 20, 21, 23, 25, 29, 30, 35, 36, 38, 40, 44, 45, 50, 51, 53, 54, 56, 57, 59, 63, 64, 69, 70, 72, 75, 80, 84, 90, 91, \n",
    "                       93, 99, 101, 106, 110, 111, 116, 117, 119, 125, 126, 131, 132, 134, 145, 149, 151, 152, 153, 157, 158, 163, 164, 168, 172, 178, 179, 181, \n",
    "                       183, 187, 188, 193, 194, 196, 198, 202, 203, 208, 209, 211, 212, 213, 218, 220, 221, 225, 235, 236, 238, 239, 240, 242, 243, 244, 249, 253, \n",
    "                       254, 259, 260, 262, 268, 274, 277, 283, 289, 292, 293, 294, 298, 299, 304, 305, 308, 309, 310, 311]\n",
    "\n",
    "# Read and filter the lines\n",
    "with open('/home/mattyshen/iCBM/CUB/CUB_200_2011/attributes/attributes.txt', \"r\") as infile, open('/home/mattyshen/iCBM/CUB/CUB_200_2011/attributes/filtered_attributes.txt', \"w\") as outfile:\n",
    "    i=1\n",
    "    for line in infile:\n",
    "        # Extract the number at the start of the line\n",
    "        line_number = int(line.split()[0])\n",
    "        # If the number is in the list, write it to the output file\n",
    "        if line_number in numbers_to_keep:\n",
    "            print(i)\n",
    "            renumbered_line = f\"{i} {' '.join(line.split()[1:])}\\n\"\n",
    "            outfile.write(renumbered_line)\n",
    "            i= i+1 \n",
    "\n",
    "print(f\"Filtered lines saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a44b28-aaed-48f4-a1eb-42403dd8ce41",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/mattyshen/iCBM/CUB/CUB_200_2011/attributes/image_attribute_labels.txt', 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i < 313:\n",
    "            file_idx, attribute_idx, attribute_label, attribute_certainty = line.strip().split()[:4]\n",
    "            print(file_idx, attribute_idx, attribute_label, attribute_certainty)\n",
    "        \n",
    "        # attribute_label = int(attribute_label)\n",
    "        # attribute_certainty = int(attribute_certainty)\n",
    "        # uncertain_label = uncertainty_map[attribute_label][attribute_certainty]\n",
    "        # attribute_labels_all[int(file_idx)].append(attribute_label)\n",
    "        # attribute_uncertain_labels_all[int(file_idx)].append(uncertain_label)\n",
    "        # attribute_certainties_all[int(file_idx)].append(attribute_certainty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db1e5bc-1929-49b2-819a-1d75c8bd9cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = [1, 3, 5] #top k class accuracies to compute\n",
    "\n",
    "def eval(args):\n",
    "    \"\"\"\n",
    "    Run inference using model (and model2 if bottleneck)\n",
    "    Returns: (for notebook analysis)\n",
    "    all_class_labels: flattened list of class labels for each image\n",
    "    topk_class_outputs: array of top k class ids predicted for each image. Shape = size of test set * max(K)\n",
    "    all_class_outputs: array of all logit outputs for class prediction, shape = N_TEST * N_CLASS\n",
    "    all_attr_labels: flattened list of labels for each attribute for each image (length = N_ATTRIBUTES * N_TEST)\n",
    "    all_attr_outputs: flatted list of attribute logits (after ReLU/ Sigmoid respectively) predicted for each attribute for each image (length = N_ATTRIBUTES * N_TEST)\n",
    "    all_attr_outputs_sigmoid: flatted list of attribute logits predicted (after Sigmoid) for each attribute for each image (length = N_ATTRIBUTES * N_TEST)\n",
    "    wrong_idx: image ids where the model got the wrong class prediction (to compare with other models)\n",
    "    \"\"\"\n",
    "    if args.model_dir:\n",
    "        model = torch.load(args.model_dir)\n",
    "        \n",
    "    else:\n",
    "        model = None\n",
    "\n",
    "    if not hasattr(model, 'use_relu'):\n",
    "        if args.use_relu:\n",
    "            model.use_relu = True\n",
    "        else:\n",
    "            model.use_relu = False\n",
    "    if not hasattr(model, 'use_sigmoid'):\n",
    "        if args.use_sigmoid:\n",
    "            model.use_sigmoid = True\n",
    "        else:\n",
    "            model.use_sigmoid = False\n",
    "    if not hasattr(model, 'cy_fc'):\n",
    "        model.cy_fc = None\n",
    "    model = model.to(get_device())\n",
    "    model.eval()\n",
    "\n",
    "    if args.model_dir2:\n",
    "        if 'rf' in args.model_dir2:\n",
    "            model2 = joblib.load(args.model_dir2)\n",
    "        else:\n",
    "            model2 = torch.load(args.model_dir2)\n",
    "        if not hasattr(model2, 'use_relu'):\n",
    "            if args.use_relu:\n",
    "                model2.use_relu = True\n",
    "            else:\n",
    "                model2.use_relu = False\n",
    "        if not hasattr(model2, 'use_sigmoid'):\n",
    "            if args.use_sigmoid:\n",
    "                model2.use_sigmoid = True\n",
    "            else:\n",
    "                model2.use_sigmoid = False\n",
    "        model2 = model2.to(get_device())\n",
    "        model2.eval()\n",
    "    else:\n",
    "        model2 = None\n",
    "\n",
    "    if args.use_attr:\n",
    "        attr_acc_meter = [AverageMeter()]\n",
    "        if args.feature_group_results:  # compute acc for each feature individually in addition to the overall accuracy\n",
    "            for _ in range(args.n_attributes):\n",
    "                attr_acc_meter.append(AverageMeter())\n",
    "    else:\n",
    "        attr_acc_meter = None\n",
    "\n",
    "    class_acc_meter = []\n",
    "    for j in range(len(K)):\n",
    "        class_acc_meter.append(AverageMeter())\n",
    "    if args.eval_data == 'trainval':\n",
    "        train_dir = data_dir = os.path.join(BASE_DIR, args.data_dir, 'train.pkl')\n",
    "        val_dir = data_dir = os.path.join(BASE_DIR, args.data_dir, 'val.pkl')\n",
    "        loader = load_data([train_dir, val_dir], args.use_attr, args.no_img, args.batch_size, image_dir=args.image_dir,\n",
    "                           n_class_attr=args.n_class_attr)\n",
    "    else:\n",
    "        data_dir = os.path.join(BASE_DIR, args.data_dir, args.eval_data + '.pkl')\n",
    "        loader = load_data([data_dir], args.use_attr, args.no_img, args.batch_size, image_dir=args.image_dir,\n",
    "                           n_class_attr=args.n_class_attr)\n",
    "    all_outputs, all_targets = [], []\n",
    "    all_attr_labels, all_attr_outputs, all_attr_outputs_sigmoid, all_attr_outputs2 = [], [], [], []\n",
    "    all_class_labels, all_class_outputs, all_class_logits = [], [], []\n",
    "    topk_class_labels, topk_class_outputs = [], []\n",
    "\n",
    "    for data_idx, data in enumerate(loader):\n",
    "        if args.use_attr:\n",
    "            if args.no_img:  # A -> Y\n",
    "                inputs, labels = data\n",
    "                if isinstance(inputs, list):\n",
    "                    inputs = torch.stack(inputs).t().float()\n",
    "                inputs = inputs.float()\n",
    "                # inputs = torch.flatten(inputs, start_dim=1).float()\n",
    "            else:\n",
    "                inputs, labels, attr_labels = data\n",
    "                attr_labels = torch.stack(attr_labels).t()  # N x 312\n",
    "        else:  # simple finetune\n",
    "            inputs, labels = data\n",
    "\n",
    "        inputs_var = torch.autograd.Variable(inputs).to(get_device())\n",
    "        labels_var = torch.autograd.Variable(labels).to(get_device())\n",
    "        labels = labels.to(get_device()) if torch.cuda.is_available() else labels\n",
    "\n",
    "        if args.attribute_group:\n",
    "            outputs = []\n",
    "            f = open(args.attribute_group, 'r')\n",
    "            for line in f:\n",
    "                attr_model = torch.load(line.strip())\n",
    "                outputs.extend(attr_model(inputs_var))\n",
    "        else:\n",
    "            outputs = model(inputs_var)\n",
    "        if args.use_attr:\n",
    "            if args.no_img:  # A -> Y\n",
    "                class_outputs = outputs\n",
    "            else:\n",
    "                if args.bottleneck:\n",
    "                    if args.use_relu:\n",
    "                        attr_outputs = [torch.nn.ReLU()(o) for o in outputs]\n",
    "                        attr_outputs_sigmoid = [torch.nn.Sigmoid()(o) for o in outputs]\n",
    "                    elif args.use_sigmoid:\n",
    "                        attr_outputs = [torch.nn.Sigmoid()(o) for o in outputs]\n",
    "                        attr_outputs_sigmoid = attr_outputs\n",
    "                    else:\n",
    "                        attr_outputs = outputs\n",
    "                        attr_outputs_sigmoid = [torch.nn.Sigmoid()(o) for o in outputs]\n",
    "                    if model2:\n",
    "                        stage2_inputs = torch.cat(attr_outputs, dim=1)\n",
    "                        class_outputs = model2(stage2_inputs)\n",
    "                    else:  # for debugging bottleneck performance without running stage 2\n",
    "                        class_outputs = torch.zeros([inputs.size(0), N_CLASSES],\n",
    "                                                    dtype=torch.float64).to(get_device())  # ignore this\n",
    "                else:  # cotraining, end2end\n",
    "                    if args.use_relu:\n",
    "                        attr_outputs = [torch.nn.ReLU()(o) for o in outputs[1:]]\n",
    "                        attr_outputs_sigmoid = [torch.nn.Sigmoid()(o) for o in outputs[1:]]\n",
    "                    elif args.use_sigmoid:\n",
    "                        attr_outputs = [torch.nn.Sigmoid()(o) for o in outputs[1:]]\n",
    "                        attr_outputs_sigmoid = attr_outputs\n",
    "                    else:\n",
    "                        attr_outputs = outputs[1:]\n",
    "                        attr_outputs_sigmoid = [torch.nn.Sigmoid()(o) for o in outputs[1:]]\n",
    "\n",
    "                    class_outputs = outputs[0]\n",
    "\n",
    "                for i in range(args.n_attributes):\n",
    "                    acc = binary_accuracy(attr_outputs_sigmoid[i].squeeze(), attr_labels[:, i])\n",
    "                    acc = acc.data.cpu().numpy()\n",
    "                    # acc = accuracy(attr_outputs_sigmoid[i], attr_labels[:, i], topk=(1,))\n",
    "                    attr_acc_meter[0].update(acc, inputs.size(0))\n",
    "                    if args.feature_group_results:  # keep track of accuracy of individual attributes\n",
    "                        attr_acc_meter[i + 1].update(acc, inputs.size(0))\n",
    "\n",
    "                attr_outputs = torch.cat([o.unsqueeze(1) for o in attr_outputs], dim=1)\n",
    "                attr_outputs_sigmoid = torch.cat([o for o in attr_outputs_sigmoid], dim=1)\n",
    "                all_attr_outputs.extend(list(attr_outputs.flatten().data.cpu().numpy()))\n",
    "                all_attr_outputs_sigmoid.extend(list(attr_outputs_sigmoid.flatten().data.cpu().numpy()))\n",
    "                all_attr_labels.extend(list(attr_labels.flatten().data.cpu().numpy()))\n",
    "        else:\n",
    "            class_outputs = outputs[0]\n",
    "\n",
    "        _, topk_preds = class_outputs.topk(max(K), 1, True, True)\n",
    "        _, preds = class_outputs.topk(1, 1, True, True)\n",
    "        all_class_outputs.extend(list(preds.detach().cpu().numpy().flatten()))\n",
    "        all_class_labels.extend(list(labels.data.cpu().numpy()))\n",
    "        all_class_logits.extend(class_outputs.detach().cpu().numpy())\n",
    "        topk_class_outputs.extend(topk_preds.detach().cpu().numpy())\n",
    "        topk_class_labels.extend(labels.view(-1, 1).expand_as(preds))\n",
    "\n",
    "        np.set_printoptions(threshold=sys.maxsize)\n",
    "        class_acc = accuracy(class_outputs, labels, topk=K)  # only class prediction accuracy\n",
    "        for m in range(len(class_acc_meter)):\n",
    "            class_acc_meter[m].update(class_acc[m], inputs.size(0))\n",
    "\n",
    "    all_class_logits = np.vstack(all_class_logits)\n",
    "    topk_class_outputs = np.vstack([tco if isinstance(tco, np.ndarray) else tco.cpu() for tco in topk_class_outputs])\n",
    "    topk_class_labels = np.vstack([tcl if isinstance(tcl, np.ndarray) else tcl.cpu() for tcl in topk_class_labels])\n",
    "    wrong_idx = np.where(np.sum(topk_class_outputs == topk_class_labels, axis=1) == 0)[0]\n",
    "\n",
    "    for j in range(len(K)):\n",
    "        print('Average top %d class accuracy: %.5f' % (K[j], class_acc_meter[j].avg))\n",
    "\n",
    "    if args.use_attr and not args.no_img:  # print some metrics for attribute prediction performance\n",
    "        print('Average attribute accuracy: %.5f' % attr_acc_meter[0].avg)\n",
    "        all_attr_outputs_int = np.array(all_attr_outputs_sigmoid) >= 0.5\n",
    "        if args.feature_group_results:\n",
    "            n = len(all_attr_labels)\n",
    "            all_attr_acc, all_attr_f1 = [], []\n",
    "            for i in range(args.n_attributes):\n",
    "                acc_meter = attr_acc_meter[1 + i]\n",
    "                attr_acc = float(acc_meter.avg)\n",
    "                attr_preds = [all_attr_outputs_int[j] for j in range(n) if j % args.n_attributes == i]\n",
    "                attr_labels = [all_attr_labels[j] for j in range(n) if j % args.n_attributes == i]\n",
    "                attr_f1 = f1_score(attr_labels, attr_preds)\n",
    "                all_attr_acc.append(attr_acc)\n",
    "                all_attr_f1.append(attr_f1)\n",
    "\n",
    "            '''\n",
    "            fig, axs = plt.subplots(1, 2, figsize=(20,10))\n",
    "            for plt_id, values in enumerate([all_attr_acc, all_attr_f1]):\n",
    "                axs[plt_id].set_xticks(np.arange(0, 1.1, 0.1))\n",
    "                if plt_id == 0:\n",
    "                    axs[plt_id].hist(np.array(values)/100.0, bins=np.arange(0, 1.1, 0.1), rwidth=0.8)\n",
    "                    axs[plt_id].set_title(\"Attribute accuracies distribution\")\n",
    "                else:\n",
    "                    axs[plt_id].hist(values, bins=np.arange(0, 1.1, 0.1), rwidth=0.8)\n",
    "                    axs[plt_id].set_title(\"Attribute F1 scores distribution\")\n",
    "            plt.savefig('/'.join(args.model_dir.split('/')[:-1]) + '.png')\n",
    "            '''\n",
    "            bins = np.arange(0, 1.01, 0.1)\n",
    "            acc_bin_ids = np.digitize(np.array(all_attr_acc) / 100.0, bins)\n",
    "            acc_counts_per_bin = [np.sum(acc_bin_ids == (i + 1)) for i in range(len(bins))]\n",
    "            f1_bin_ids = np.digitize(np.array(all_attr_f1), bins)\n",
    "            f1_counts_per_bin = [np.sum(f1_bin_ids == (i + 1)) for i in range(len(bins))]\n",
    "            print(\"Accuracy bins:\")\n",
    "            print(acc_counts_per_bin)\n",
    "            print(\"F1 bins:\")\n",
    "            print(f1_counts_per_bin)\n",
    "            np.savetxt(os.path.join(args.log_dir, 'concepts.txt'), f1_counts_per_bin)\n",
    "\n",
    "        balanced_acc, report = multiclass_metric(all_attr_outputs_int, all_attr_labels)\n",
    "        f1 = f1_score(all_attr_labels, all_attr_outputs_int)\n",
    "        print(\"Total 1's predicted:\", sum(np.array(all_attr_outputs_sigmoid) >= 0.5) / len(all_attr_outputs_sigmoid))\n",
    "        print('Avg attribute balanced acc: %.5f' % (balanced_acc))\n",
    "        print(\"Avg attribute F1 score: %.5f\" % f1)\n",
    "        print(report + '\\n')\n",
    "    return class_acc_meter, attr_acc_meter, all_class_labels, topk_class_outputs, all_class_logits, all_attr_labels, all_attr_outputs, all_attr_outputs_sigmoid, wrong_idx, all_attr_outputs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6884a92a-a877-4bb7-9db0-10ddbde6c3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_results, c_results = [], []\n",
    "for i, model_dir in enumerate(args.model_dirs):\n",
    "    args.model_dir = model_dir\n",
    "    args.model_dir2 = args.model_dirs2[i] if args.model_dirs2 else None\n",
    "    result = eval(args)\n",
    "    class_acc_meter, attr_acc_meter = result[0], result[1]\n",
    "    y_results.append(1 - class_acc_meter[0].avg[0].item() / 100.)\n",
    "    if attr_acc_meter is not None:\n",
    "        c_results.append(1 - attr_acc_meter[0].avg.item() / 100.)\n",
    "    else:\n",
    "        c_results.append(-1)\n",
    "values = (np.mean(y_results), np.std(y_results), np.mean(c_results), np.std(c_results))\n",
    "output_string = '%.4f %.4f %.4f %.4f' % values\n",
    "print_string = 'Error of y: %.4f +- %.4f, Error of C: %.4f +- %.4f' % values\n",
    "print(print_string)\n",
    "f = open(os.path.join(args.log_dir, 'results.txt'), \"a\")\n",
    "f.write(output_string)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3ae1fe-ef4f-4be6-acf1-ddfb4d06e1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(os.path.join(args.log_dir, 'results.txt'), \"a\")\n",
    "f.write(output_string)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bc0664-571e-428d-834a-d69894e86a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d2930d-b39e-4967-b3bb-3609fd0ebdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(args.log_dir, 'results.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17657b9b-c4e4-412e-8297-f372c6a3f2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df44f78-10b2-4095-8f87-d5bcc2c295ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig(x):\n",
    "    return 1/(1+np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa5ae35-45b9-4057-bf47-ee072a0db1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(os.path.join(args.log_dir, 'results.txt'), \"r\")\n",
    "content = file.read()\n",
    "print(content)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea96cda-dbff-4a9f-a003-97047641256a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
